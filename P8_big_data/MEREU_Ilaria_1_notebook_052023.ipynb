{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d88eb78c-4003-4c40-8b40-a04b74b5c2f7",
   "metadata": {},
   "source": [
    "# Déployez un modèle dans le cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36206e1a-2cbd-4dd8-94b1-8912e194b00f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import sagemaker_pyspark\n",
    "\n",
    "conf = (SparkConf()\n",
    "        .set(\"spark.driver.extraClassPath\", \":\".join(sagemaker_pyspark.classpath_jars())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bcad037-c730-41ec-bf8f-6652d33919c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/27 13:55:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "             .builder\n",
    "             .config(conf=conf)\n",
    "             .appName('P8')\n",
    "             .master('local')\n",
    "             .config(\"spark.sql.parquet.writeLegacyFormat\", 'true') # OC\n",
    "             .config('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem') # MC\n",
    "             .appName(\"Fruit Image Classification\")\n",
    "             .getOrCreate()\n",
    ")\n",
    "\n",
    "# SparkContext(conf=conf)\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0fd2607-8a23-4ef7-8839-8a4cd37475db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://sagemaker-pyspark.readthedocs.io/en/latest/\n",
    "from sagemaker_pyspark import IAMRole\n",
    "\n",
    "iam_role = \"arn:aws:iam::319752300128:role/service-role/SageMaker-DS2\"\n",
    "\n",
    "region = \"eu-central-1\"\n",
    "# training_data = spark.read.format(\"libsvm\").option(\"numFeatures\", \"784\")\n",
    "  # .load(\"s3a://sagemaker-sample-data-{}/spark/mnist/train/\".format(region))\n",
    "\n",
    "S3_bucket='p8-images'\n",
    "data_key = 'sample'\n",
    "data_location = 's3a://{}/{}'.format(S3_bucket, data_key)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec604479-913d-409d-8245-839446650753",
   "metadata": {},
   "source": [
    "# Séléction des blocs à activer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "992c44ab-c257-4d60-ab0d-9cd52416bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testés\n",
    "seq_actions = ['lect_2', 'feat_2'] # sequence pour PCA ok\n",
    "# seq_actions = ['lect_1', 'feat_1'] # OOM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0645df8e-2470-4c33-b00d-ecd65cc174b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lecture sur s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "479759ee-c68c-4d7c-a536-40f5f45b0f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0c8b98-cac4-4207-a8a1-df913c5770c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Méthode de lecture 1 (\"alternant\")\n",
    "spark.read / IAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8eb210c-8f63-4f86-8396-596c206a297d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split\n",
    "\n",
    "if 'lect_1' in seq_actions:\n",
    "    # lecture des ressources via spark / sagemaker_pypsark - approche \"alternant\"\n",
    "    images = spark.read.format(\"binaryFile\").option(\"pathGlobFilter\", \"*.jpg\").option(\"recursiveFileLookup\", \"true\").load(data_location)\n",
    "    # https://sagemaker-pyspark.readthedocs.io/en/latest/\n",
    "    # training_data = spark.read.format(\"libsvm\").option(\"numFeatures\", \"784\")\n",
    "      # .load(\"s3a://sagemaker-sample-data-{}/spark/mnist/train/\".format(region))\n",
    "\n",
    "    print(images)\n",
    "    \n",
    "    images = images.withColumn('label', element_at(split(images['path'], '/'),-2))\n",
    "    print(images.printSchema())\n",
    "    print(images.select('path','label').show(5,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f938875e-9f86-4f64-9014-af5c35c1b66b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Méthode de lecture 2\n",
    "boto3 (\"lect_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23549973-bbf1-4d57-b936-66ee47daabc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p8-images\n",
      "Categorie : ['Kumquats'], sample/Kumquats/r_312_100.jpg\n",
      "Categorie : ['Kumquats'], sample/Kumquats/r_313_100.jpg\n",
      "Categorie : ['Kumquats'], sample/Kumquats/r_314_100.jpg\n",
      "Categorie : ['Kumquats'], sample/Kumquats/r_3_100.jpg\n",
      "Categorie : ['Kumquats'], sample/Kumquats/r_4_100.jpg\n",
      "Categorie : ['Nectarine'], sample/Nectarine/r_92_100.jpg\n",
      "Categorie : ['Nectarine'], sample/Nectarine/r_93_100.jpg\n",
      "Categorie : ['Nectarine'], sample/Nectarine/r_94_100.jpg\n",
      "Categorie : ['Nectarine'], sample/Nectarine/r_95_100.jpg\n",
      "Categorie : ['Nectarine'], sample/Nectarine/r_96_100.jpg\n",
      "Categorie : ['Kumquats'], sample/Kumquats/r_312_100.jpg\n",
      "Categorie : ['Kumquats'], sample/Kumquats/r_313_100.jpg\n",
      "Categorie : ['Kumquats'], sample/Kumquats/r_314_100.jpg\n",
      "Categorie : ['Kumquats'], sample/Kumquats/r_3_100.jpg\n",
      "Categorie : ['Kumquats'], sample/Kumquats/r_4_100.jpg\n",
      "Categorie : ['Nectarine'], sample/Nectarine/r_92_100.jpg\n",
      "Categorie : ['Nectarine'], sample/Nectarine/r_93_100.jpg\n",
      "Categorie : ['Nectarine'], sample/Nectarine/r_94_100.jpg\n",
      "Categorie : ['Nectarine'], sample/Nectarine/r_95_100.jpg\n",
      "Categorie : ['Nectarine'], sample/Nectarine/r_96_100.jpg\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- path: string (nullable = true)\n",
      "\n",
      "+-----------------------------+\n",
      "|path                         |\n",
      "+-----------------------------+\n",
      "|sample/Kumquats/r_312_100.jpg|\n",
      "|sample/Kumquats/r_313_100.jpg|\n",
      "|sample/Kumquats/r_314_100.jpg|\n",
      "|sample/Kumquats/r_3_100.jpg  |\n",
      "|sample/Kumquats/r_4_100.jpg  |\n",
      "|sample/Nectarine/r_92_100.jpg|\n",
      "|sample/Nectarine/r_93_100.jpg|\n",
      "|sample/Nectarine/r_94_100.jpg|\n",
      "|sample/Nectarine/r_95_100.jpg|\n",
      "|sample/Nectarine/r_96_100.jpg|\n",
      "+-----------------------------+\n",
      "\n",
      "+-----------------------------+---------+\n",
      "|path                         |categorie|\n",
      "+-----------------------------+---------+\n",
      "|sample/Kumquats/r_312_100.jpg|Kumquats |\n",
      "|sample/Kumquats/r_313_100.jpg|Kumquats |\n",
      "|sample/Kumquats/r_314_100.jpg|Kumquats |\n",
      "|sample/Kumquats/r_3_100.jpg  |Kumquats |\n",
      "|sample/Kumquats/r_4_100.jpg  |Kumquats |\n",
      "+-----------------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- categorie: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "import boto3 # S3 bucket\n",
    "\n",
    "OUT_FOLDER='Results'\n",
    "\n",
    "print(S3_bucket)\n",
    "s3 = boto3.resource('s3')\n",
    "bucket=s3.Bucket(S3_bucket)\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "if 'lect_2' in seq_actions:\n",
    "    file_iterator= bucket.objects.filter(Prefix=data_key)\n",
    "    for f in file_iterator.limit(10):\n",
    "        categorie=f.key.split('/')[-2:-1]\n",
    "        print(f'Categorie : {categorie}, {f.key}')\n",
    "\n",
    "    file_iterator= bucket.objects.filter(Prefix=data_key)\n",
    "    for f in file_iterator.limit(10):\n",
    "        categorie=f.key.split('/')[-2:-1]\n",
    "        print(f'Categorie : {categorie}, {f.key}')    \n",
    "    filelist=[]\n",
    "    for file in file_iterator:\n",
    "        filelist.append(file.key)\n",
    "\n",
    "    # sc= spark.sparkContext\n",
    "\n",
    "    filelist_rdd = sc.parallelize(filelist, numSlices=4)\n",
    "    print (type(filelist_rdd))\n",
    "    print(filelist_rdd.getNumPartitions())\n",
    "\n",
    "    filelist_df = filelist_rdd.map(lambda x:Row(x)).toDF(['path'])\n",
    "    print(type(filelist_df))\n",
    "    filelist_df.printSchema()\n",
    "    filelist_df.limit(10).show(truncate=False)\n",
    "    df_images = (filelist_df.withColumn('categorie',F.element_at(F.split(filelist_df['path'],\"/\"),2)))\n",
    "    df_images.show(5,False)\n",
    "    df_images.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7afe27c-bf9f-45fe-bb58-38f03d463671",
   "metadata": {},
   "source": [
    "# Featurisation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8cfa73-0656-4707-92a8-10ee2f286a2c",
   "metadata": {},
   "source": [
    "## Méthode 1 (\"alternant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6a21d02-ffce-4273-a459-49e42ce49fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59f27573-7250-4178-b1b6-23c137e2f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import VectorUDT, Vectors\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# sc = spark.sparkContext\n",
    "\n",
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "# @udf(returnType=VectorUDT())\n",
    "# def featurize_udf1(content_series_iter):\n",
    "#     for content_series in content_series_iter:\n",
    "#       content_series = pd.Series(content_series.tolist())\n",
    "#       yield featurize_series(model, content_series)\n",
    "\n",
    "from tensorflow.keras.applications import vgg16\n",
    "\n",
    "def create_model():\n",
    "    # vgg16.VGG16\n",
    "    # MobileNetV2\n",
    "    model = vgg16.VGG16(weights='imagenet',\n",
    "                        include_top=True,\n",
    "                        input_shape=(3, 224, 224) #sagemaker vgg16\n",
    "                        # input_shape=(224, 224, 3) # colab\n",
    "                        )\n",
    "\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    return model\n",
    "\n",
    "# @udf(returnType=VectorUDT())\n",
    "def featurize(content):\n",
    "    # Load the model weights inside the UDF\n",
    "    model = create_model()\n",
    "    # Load the model weights\n",
    "    # model.load_weights(model_weights.value)\n",
    "    model.layers.pop()  # Remove the last layer\n",
    "    model = Model(inputs=model.input, outputs=model.layers[-1].output)\n",
    "\n",
    "    features = featurize_series(model, content)\n",
    "    del model\n",
    "    return pd.Series([Vectors.dense(f) for f in features])\n",
    "\n",
    "featurize_RN_udf = udf(featurize, VectorUDT())\n",
    "\n",
    "if 'feat_1' in seq_actions:\n",
    "    model = create_model()\n",
    "    model_weights = sc.broadcast(model.get_weights())\n",
    "    \n",
    "    # image_features = images.repartition(8000).select(col(\"path\"), col(\"label\"), featurize_RN_udf(col(\"content\")).alias(\"features_images\"))\n",
    "    image_features = images.repartition(15000).select(col(\"path\"), col(\"label\"), featurize_RN_udf(col(\"content\")).alias(\"features_images\"))\n",
    "    \n",
    "    image_features.printSchema()   \n",
    "    \n",
    "    # udf_vectorized = udf(lambda r: Vectors.dense(r), VectorUDT())\n",
    "    # image_features = image_features.repartition(15000).withColumn(\"vector_data\", )\n",
    "    # image_features = image_features.repartition(15000).select(col(\"path\"), col(\"label\"), udf_vectorized(F.col(\"features_images\")).alias(\"features_images\"))\n",
    "    # image_features.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22691564-a54d-4831-899e-29121b149214",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'feat_1' in seq_actions:\n",
    "    \n",
    "    from pyspark.ml.feature import PCA\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=['features_images'], outputCol='features_images_2')\n",
    "    # assembler.transform(image_features)\n",
    " \n",
    "    scaler = StandardScaler(inputCol='features_images_2', outputCol='scaled_features', withStd=True, withMean=True)\n",
    "    # model = scaler.fit(image_features)\n",
    "    # df_scaled = model.transform(image_features)\n",
    "\n",
    "# Réduction de dimensions\n",
    "    # pca = PCA(k=30, inputCol='scaled_features', outputCol='pca_features')\n",
    "    # pca_model = pca.fit(df_scaled)\n",
    "    # df_pca_features = pca_model.transform(df_scaled)\n",
    "    # df_pca_features.show()\n",
    "\n",
    "    pca = PCA(k=2, inputCol='scaled_features', outputCol='pcaFeature')\n",
    "    # pipeline = Pipeline(stages=[scaler,pca])\n",
    "    pipeline = Pipeline(stages=[assembler,scaler,pca])\n",
    "    model = pipeline.fit(image_features)\n",
    "    prediction = model.transform(image_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70334aee-16ce-47ce-872f-2a32afbcae2d",
   "metadata": {},
   "source": [
    "L'opération de normalisation est acceptée, mais elle s'arrête pour un problème qui s'associe à une insuffisance de mémoire :\n",
    "\"Answer from Java side is empty\" -> \n",
    "https://stackoverflow.com/questions/37252809/apache-spark-py4jerror-answer-from-java-side-is-empty#40803994\n",
    "    \"Usually, you'll see this error when the Java process get silently killed by the OOM Killer.\"\n",
    "https://stackoverflow.com/questions/21138751/spark-java-lang-outofmemoryerror-java-heap-space/22742982#22742982\n",
    "    \"Try using more partitions, you should have 2 - 4 per CPU. IME increasing the number of partitions is often the easiest way to make a program more stable (and often faster). For huge amounts of data you may need way more than 4 per CPU, I've had to use 8000 partitions in some cases!\"\n",
    "\n",
    "Cette approche semble donc être correcte, mais nécessiter d'une configuration de niveau avancé pour aboutir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55eff3-4393-4e57-b60d-9912ca9c9cf0",
   "metadata": {},
   "source": [
    "## Méthode 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15244ac7-c113-414e-ac9f-3ac1eccc7fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- categorie: string (nullable = true)\n",
      " |-- binary_data: string (nullable = true)\n",
      " |-- vector_data: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import VectorUDT, Vectors\n",
    "from PIL import Image\n",
    "\n",
    "def load_binary(file_key):\n",
    "    \"\"\"generic method to get any object from S3\"\"\"\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket= s3.Bucket(S3_bucket)\n",
    "    #print('file_key',file_key)\n",
    "    obj = bucket.Object(key=file_key)\n",
    "    response=obj.get()\n",
    "    im =response['Body']\n",
    "    \n",
    "    img =Image.open(im).resize([16,16])\n",
    "    \n",
    "    return np.array(img).flatten().tolist()\n",
    "\n",
    "if 'feat_2' in seq_actions:\n",
    "    udf_binary= F.udf(load_binary)\n",
    "    df_images = df_images.withColumn('binary_data', udf_binary(F.col('path')))\n",
    "    udf_vectorized = udf(lambda r: Vectors.dense(r), VectorUDT())\n",
    "    df_images = df_images.withColumn(\"vector_data\", udf_vectorized(F.col(\"binary_data\")))\n",
    "    df_images.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ea22c4-5aba-4b5d-ba3e-77ca362f6ff7",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26e1c126-637d-42bf-9a9d-afdb4d67bc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/27 13:56:44 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/06/27 13:56:44 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/27 13:56:49 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "23/06/27 13:56:49 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+--------------------+--------------------+\n",
      "|                path|  categorie|            features|     scaled_features|        pca_features|\n",
      "+--------------------+-----------+--------------------+--------------------+--------------------+\n",
      "|sample/Kumquats/r...|   Kumquats|[255.0,255.0,255....|[0.46349236760188...|[-30.166743432983...|\n",
      "|sample/Kumquats/r...|   Kumquats|[255.0,255.0,255....|[0.46349236760188...|[-30.314354030257...|\n",
      "|sample/Kumquats/r...|   Kumquats|[255.0,255.0,255....|[0.46349236760188...|[-30.675945989220...|\n",
      "|sample/Kumquats/r...|   Kumquats|[255.0,255.0,255....|[0.46349236760188...|[-28.206877298210...|\n",
      "|sample/Kumquats/r...|   Kumquats|[255.0,255.0,255....|[0.46349236760188...|[-28.507394301920...|\n",
      "|sample/Nectarine/...|  Nectarine|[255.0,255.0,255....|[0.46349236760188...|[24.6217297604019...|\n",
      "|sample/Nectarine/...|  Nectarine|[255.0,255.0,255....|[0.46349236760188...|[25.0600587856122...|\n",
      "|sample/Nectarine/...|  Nectarine|[255.0,255.0,255....|[0.46349236760188...|[24.9293447587457...|\n",
      "|sample/Nectarine/...|  Nectarine|[255.0,255.0,255....|[0.46349236760188...|[25.0952793372812...|\n",
      "|sample/Nectarine/...|  Nectarine|[255.0,255.0,255....|[0.46349236760188...|[25.7245922479320...|\n",
      "|sample/Orange/r_9...|     Orange|[254.0,255.0,252....|[-0.9849212811539...|[9.50425287877497...|\n",
      "|sample/Orange/r_9...|     Orange|[253.0,254.0,252....|[-2.4333349299097...|[9.60718838678243...|\n",
      "|sample/Orange/r_9...|     Orange|[253.0,254.0,252....|[-2.4333349299097...|[9.73137346835207...|\n",
      "|sample/Orange/r_9...|     Orange|[254.0,254.0,252....|[-0.9849212811539...|[9.72954407596274...|\n",
      "|sample/Orange/r_9...|     Orange|[253.0,254.0,252....|[-2.4333349299097...|[10.1888075068829...|\n",
      "|sample/Pomegranat...|Pomegranate|[255.0,255.0,255....|[0.46349236760188...|[-2.3012142030667...|\n",
      "|sample/Pomegranat...|Pomegranate|[255.0,255.0,255....|[0.46349236760188...|[-2.4598434971334...|\n",
      "|sample/Pomegranat...|Pomegranate|[255.0,255.0,255....|[0.46349236760188...|[-5.4953993957072...|\n",
      "|sample/Pomegranat...|Pomegranate|[255.0,255.0,255....|[0.46349236760188...|[-5.6887811799603...|\n",
      "|sample/Pomegranat...|Pomegranate|[255.0,255.0,255....|[0.46349236760188...|[-5.5965921442114...|\n",
      "+--------------------+-----------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "\n",
    "if 'feat_2' in seq_actions:\n",
    "    df_features=df_images.select(col('path'),col('categorie'),col('vector_data').alias('features'))\n",
    "    \n",
    "    # # Normalisation\n",
    "    scaler = StandardScaler(inputCol='features', outputCol='scaled_features', withStd=True, withMean=True)\n",
    "    model = scaler.fit(df_features)\n",
    "    df_scaled = model.transform(df_features)\n",
    "\n",
    "    # # Réduction de dimensions\n",
    "    pca = PCA(k=30, inputCol='scaled_features', outputCol='pca_features')\n",
    "    pca_model = pca.fit(df_scaled)\n",
    "    df_pca_features = pca_model.transform(df_scaled)\n",
    "    df_pca_features.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ee2bcf-82ae-4194-992d-829ed02b64a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sauvegarde résultat su s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18e6169e-996b-4918-bc6c-5f92989a358b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 25 records to Results/resultats_pca.csv on bucket s3a://p8-images\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "def write_dataframe_to_csv_on_s3(dataframe, bucket, filename):\n",
    "    \"\"\" Write a dataframe to a CSV on S3 \"\"\"\n",
    "    # Create buffer\n",
    "    csv_buffer = io.StringIO()\n",
    "    # Write dataframe to buffer\n",
    "    dataframe.to_csv(csv_buffer, sep=\",\", header=None,index=None)\n",
    "    # Create S3 object\n",
    "    s3_resource = boto3.resource(\"s3\")\n",
    "    # Write buffer to S3 object\n",
    "    s3_resource.Object(bucket, filename).put(Body=csv_buffer.getvalue())\n",
    "    print(f'Writing {len(dataframe)} records to {filename} on bucket s3a://{bucket}')\n",
    "\n",
    "if 'feat_2' in seq_actions:\n",
    "    df_pandas = df_pca_features.toPandas()\n",
    "    df_pandas.head()\n",
    "    write_dataframe_to_csv_on_s3(df_pandas, S3_bucket, f'{OUT_FOLDER}/resultats_pca.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff523f9-839d-4717-88b5-172dda64cf4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PCA: variance expliquée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fd1456e-5a24-4e14-8b68-7610b3e4177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_exppvar_plot(pca_: PCA, label: str = None):\n",
    "    \"\"\"\n",
    "    Display a plot with cumulative explained variance.\n",
    "    Parameters\n",
    "    ----------\n",
    "    pca_    fitted PCA decomposition\n",
    "    \"\"\"\n",
    "    ### \"\"\"Display a scree plot for the pca\"\"\"\n",
    "    scree_ = pca_.explainedVariance * 100\n",
    "    #print(np.round(np.cumsum(scree_), 1))\n",
    "    plt.bar(np.arange(len(scree_)) + 1, scree_,)\n",
    "    plt.plot(np.arange(len(scree_)) + 1,\n",
    "             scree_.cumsum(), color='r', marker='o', label=label)\n",
    "    # plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.title('Scree Plot of PCA: Component Eigenvalues')\n",
    "    plt.axhline(y=95,linewidth=0.5, color='k', ls='--', label='95%')\n",
    "    plt.xlabel(\"Number of principal components\")\n",
    "    plt.ylabel(\"Percentage explained variance\")\n",
    "    plt.title(\"Scree plot\")\n",
    "    \n",
    "# set feat_1\n",
    "# display_exppvar_plot(pca_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
